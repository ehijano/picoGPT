{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length 1115394\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/input.txt\",'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Text length {len(text)}\")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Character to Integer\n",
    "ctoi = {ch:i for i,ch in enumerate(chars)}\n",
    "# Integer to Character\n",
    "itoc = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "encode = lambda string: [ctoi[c] for c in string]\n",
    "decode = lambda list: ''.join([itoc[i] for i in list])\n",
    "\n",
    "print(encode(\"Hello World\"))\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Step 0 train loss 4.1734, test loss 4.1750\n",
      "Step 300 train loss 2.8502, test loss 2.8686\n",
      "Step 600 train loss 2.6358, test loss 2.6615\n",
      "Step 900 train loss 2.5512, test loss 2.5605\n",
      "Step 1200 train loss 2.5059, test loss 2.5103\n",
      "Step 1500 train loss 2.4535, test loss 2.4653\n",
      "Step 1800 train loss 2.4204, test loss 2.4357\n",
      "Step 2100 train loss 2.3946, test loss 2.3938\n",
      "Step 2400 train loss 2.3660, test loss 2.3864\n",
      "Step 2700 train loss 2.3499, test loss 2.3732\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HyperParams\n",
    "block_size = 8\n",
    "batch_size = 32\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iters = 200 \n",
    "n_embed = 32\n",
    "# Reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_bath(split):\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "    ix = torch.randint( len(data) - block_size, (batch_size,)  )\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_bath(\"train\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in [\"train\",\"test\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_bath(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" self attention Layer \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Not a parameter, so a buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k =  self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "        v = self.value(x)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of SA in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "\n",
    "\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # (V, E). Call takes (,) to (, ,E)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # self.sa_head = Head(n_embed) # For self attention = sa\n",
    "        if (n_embed % 4 != 0):\n",
    "            self.sa_head = Head(n_embed)\n",
    "            print(\"Using single SA head...\")\n",
    "        self.sa_head = MultiHeadAttention(4, int(n_embed / 4))\n",
    "        self.linear_head = nn.Linear(n_embed, vocab_size) # (E, V)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape # B = batch size, T = # of context tokens\n",
    "\n",
    "        # idx is B,T\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B, T, E)\n",
    "        position_embeddings = self.position_embedding_table( torch.arange(T, device = device) ) # (T, E)\n",
    "\n",
    "        x = token_embeddings + position_embeddings # broadcast to (B,T,E)\n",
    "\n",
    "        x = self.sa_head(x)\n",
    "\n",
    "        logits = self.linear_head(x) # (B, T, V)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Because of S.A., we can only use block size number of tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # predictions\n",
    "            logits, loss = self(idx_cond) #model call\n",
    "            logits = logits[:,-1,:] # (B,C)\n",
    "            probs = F.softmax(logits, dim = -1) # (B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # Probability sample\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # Concat\n",
    "        return idx\n",
    "    \n",
    "\n",
    "model = BigramModel()\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        ltr = losses[\"train\"]\n",
    "        ltst = losses[\"test\"]\n",
    "        print(f\"Step {iter} train loss {ltr:.4f}, test loss {ltst:.4f}\")\n",
    "\n",
    "    xb, yb = get_bath(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thourrmet hu, deinor be my anng: this, frre horwh se of sen you then thandell sop.\n",
      "\n",
      "HAlle mobaice kasotpingot therte Thisht, tha mimis by.\n",
      "To heler Iten may hur-se isbdow, danf whe-onay bivare myrare werrer aviirt be,\n",
      "Myoush, ad bor on, wand ses\n",
      "Tee, bere bpre refsacese plave hern, bonkrusonde whess sun thome yon move theas beat!\n",
      "The shing, gie il the oth:\n",
      "In.\n",
      "\n",
      "He E:\n",
      "Bathy gorme pve kist thalje bhele.\n",
      "\n",
      "Fre Rif ing non glorder dose tastes ote, peffamanoup ole eotsst she couky we; fererim hand icu\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "print(decode(\n",
    "    m.generate(context, max_new_tokens=500)[0].tolist()\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is same shape as input but takes into account previous tokens. Model will learn that some tokens are correlated to previous tokens.\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,32\n",
    "# Bath of 4\n",
    "# 8 Tokens\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Query: What we look for\n",
    "# Key: What we find\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False) # Layer takes C channels into head_size ones\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "value = nn.Linear(C, head_size, bias= False)\n",
    "\n",
    "k = key(x) # B, T, hs\n",
    "q = query(x) # B, T, hs\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # B,T,hs @ B,hs,T  --> B, T, T\n",
    "\n",
    "# Only look at past tokens\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim = 1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "\n",
    "print(\"Output is same shape as input but takes into account previous tokens. Model will learn that some tokens are correlated to previous tokens.\")\n",
    "print(out.shape)\n",
    "\n",
    "# x private information to a token\n",
    "# query - what I am interested in as a token\n",
    "# key - what information I have as a token\n",
    "# value -  what I am comunicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
