{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder Model based on the transformer in the paper \"Attention is all you need\": https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "No Encoder required, as this model generates similar text to the training dataset, intstead of performing a translation task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length 1115394\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/input.txt\",'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Text length {len(text)}\")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Character to Integer\n",
    "ctoi = {ch:i for i,ch in enumerate(chars)}\n",
    "# Integer to Character\n",
    "itoc = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "encode = lambda string: [ctoi[c] for c in string]\n",
    "decode = lambda list: ''.join([itoc[i] for i in list])\n",
    "\n",
    "print(encode(\"Hello World\"))\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 train loss 4.3869, test loss 4.3849\n",
      "Step 500 train loss 2.0203, test loss 2.1000\n",
      "Step 1000 train loss 1.6122, test loss 1.7876\n",
      "Step 1500 train loss 1.4507, test loss 1.6619\n",
      "Step 2000 train loss 1.3559, test loss 1.5707\n",
      "Step 2500 train loss 1.2912, test loss 1.5402\n",
      "Step 3000 train loss 1.2409, test loss 1.5090\n",
      "Step 3500 train loss 1.1970, test loss 1.4876\n",
      "Step 4000 train loss 1.1597, test loss 1.4898\n",
      "Step 4500 train loss 1.1261, test loss 1.4803\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HyperParams\n",
    "block_size = 256\n",
    "batch_size = 56\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iters = 200 \n",
    "n_embed = 384\n",
    "n_head = 6 \n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# Reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_bath(split):\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "    ix = torch.randint( len(data) - block_size, (batch_size,)  )\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_bath(\"train\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in [\"train\",\"test\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_bath(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" self attention Layer \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Not a parameter, so a buffer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k =  self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of SA in parallel +  projection\"\"\"\n",
    "    # Projection is a way to take the output into the original pathway. The model will learn how to do the projection.\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Simple Linear + Relu layer. Acts independently on each token, AFTER tokens have communicated.\"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed) # linear\n",
    "            , nn.ReLU() # activation\n",
    "            , nn.Linear(4*n_embed, n_embed) # projection into original pathway\n",
    "            , nn.Dropout(dropout)\n",
    "            ,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Each block implements communivation using multi-head attention, then lets each token compute separately with a feedforward. Skipping channel present for both mha and ff.\"\"\"\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.mha = MultiHeadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MyLayerNorm: # Not used. Using torch version\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim = True) # batch mean\n",
    "        xvar = x.var(1, keepdim = True) # batch std\n",
    "\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out  = self.gamma * xhat + self.beta\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # (V, E). Call takes (,) to (, ,E)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # self.sa_head = Head(n_embed) # For self attention = sa\n",
    "        if (n_embed % 4 != 0):\n",
    "            self.sa_head = Head(n_embed)\n",
    "            print(\"Using single SA head...\")\n",
    "\n",
    "        #self.sa_head = MultiHeadAttention(4, n_embed // 4)\n",
    "        #self.feedforward = FeedForward(n_embed)\n",
    "\n",
    "        self.blocks = nn.Sequential()\n",
    "        for bn in range(n_layer):\n",
    "            self.blocks.add_module(module = Block(n_embed, n_head = n_head), name = f\"Block number {bn}\" )\n",
    "        self.blocks.add_module(module = nn.LayerNorm(n_embed), name= \"Final LayerNorm\")\n",
    "\n",
    "        self.linear_head = nn.Linear(n_embed, vocab_size) # (E, V)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape # B = batch size, T = # of context tokens\n",
    "\n",
    "        # idx is B,T\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B, T, E)\n",
    "        position_embeddings = self.position_embedding_table( torch.arange(T, device = device) ) # (T, E)\n",
    "\n",
    "        x = token_embeddings + position_embeddings # broadcast to (B,T,E)\n",
    "\n",
    "        #x = self.sa_head(x)\n",
    "        #x = self.feedforward(x)\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        logits = self.linear_head(x) # (B, T, V)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Because of S.A., we can only use block size number of tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # predictions\n",
    "            logits, loss = self(idx_cond) #model call\n",
    "            logits = logits[:,-1,:] # (B,C)\n",
    "            probs = F.softmax(logits, dim = -1) # (B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # Probability sample\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # Concat\n",
    "        return idx\n",
    "    \n",
    "\n",
    "model = BigramModel()\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        ltr = losses[\"train\"]\n",
    "        ltst = losses[\"test\"]\n",
    "        print(f\"Step {iter} train loss {ltr:.4f}, test loss {ltst:.4f}\")\n",
    "\n",
    "    xb, yb = get_bath(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maken, all our woes too!\n",
      "How brings and praise his enought,\n",
      "Khathard spake your ulDishonour English?\n",
      "\n",
      "NORFOLK:\n",
      "What deceive was, what God forget?\n",
      "\n",
      "KING EDWARD IV:\n",
      "Catesby, night, what Plantagenet dies,\n",
      "For these word that Camillous' behind Northunks--\n",
      "\n",
      "KING EDWARD IV:\n",
      "So Richard in his a hor of kingly chairly father!\n",
      "\n",
      "YORK:\n",
      "O these royal father's unscient false against:\n",
      "And then our virtues are was answer'd, head,\n",
      "Whom we pronounced his ghoster.\n",
      "\n",
      "Booting Of GAUM:\n",
      "That he die: if they at old year it becals\n",
      "Have lave with some dalgried out against an after\n",
      "Leist it with sinewit. This is a late; some sworn ragis\n",
      "shall bear wond with tears of him; practises\n",
      "By ruden my wife craves, weight they gold buy life:\n",
      "But heaven, she used to the crower with gives eats,\n",
      "all the tle trouble twings of perpaint enough.\n",
      "Say, durace, when this, do determine\n",
      "acse this wear will have more of corion sighs;\n",
      "Our till I it in away or such magise!\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "But this foot wilt we my street, doth loose tord\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "print(decode(\n",
    "    m.generate(context, max_new_tokens=1000)[0].tolist()\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
